Simple parts connected by clean interfaces
	part are easily changed
	part can be written in any language, will be written in multiple languages!

Information (Debugging) is a not an afterthought 

the continuous integration framework consists of a set of small programs that can be used
	by the user, the automation system that builds components

No master 

automated installation

No database (or database that act like a file system)

build nodes are considered secured (and must be secured)
	direct access to them is subject to access control
	data access is via a secured protocol implementing access contol or 
		via encryption of the send data	
	using ssl keys

distribute the workload (no master)
	computation of what needs to be done,where can be done on client machines
	distribute the data load
		logs, reports, ... are shared from the node having the data to the user machine directly
	distribute reporting load
		only data is transfered, report generation is done on the user machine
			reports can be send back or cached or put in the binary repo
Continuous integration
	access control
		persons looking at builds, artefacts, reports, code, ... must have the right to

	build logs:
		use a build system that generates better logs

		wrap components builds in different logs
			not changing the build system just the system around it

		redirect all output in a/multiple log file

		wrap the bash, if necessary, to associate logs to a component build

		if log is hierarchical, components have separate logs, it is possible to see the 
		hierarchy and traverse it


	Reports:
		keep list of builds started locally (and some meta data)
			we may not be interested by more than those builds

		keep list of global builds
			synch with the build queue?
			only build we can access are synchronized

			how do we send our access keys?
			data send back must be encrypted
		

		build running/failed/succeeded -> location
		
		generated artefacts
			go to the binary repo
				binary repo has access control

			are accessible via scp

		presents the builds in a hierarchy (tabs, ...)

		previous builds

	triggering
		start job manually
		repo  dependencies
		build scheduling
			synchronize builds with equivalent input	

	access to workspace
		only to code that the user can checkout

	keeps credential for repos
		NOPE, uses nodes with the right credentials and get back artefacts

	UI, configure jobs
		NOPE, all config files under version control
			EG: make a branch change the config, build the branch

		if the repos are not accessible but still need a specific configuration
			the configuration can be in a separate repo that is accessible
			make a branch, checking changes, give name of branch as input to the main build

	replay
		if the repo is not accessible then replay is not accessible

		if accessible, clone the repo, rebuild the local changes
			locally if possible
			on CI-cluster otherwise
		
			optimize by using binary repo from CI-cluster



	scripting to implement build steps
		NOPE! build in the build system

		add a layer of build system if necessary

		

	scripting to control CI system
	plugins

	Agents
		multiplatform build
		pre-setup agents (build tools, ...)
			local cache
		agent configuration and management
			list of agents
				can be a share list in a repository

		find agent for a build
			the build itself can find the agent among the available agents
					

	stashing
		binary repo per build
		
		this means that multiple builds with the same configuration having the same
		stashed artefact (yes it's the same or we have a bigger problem) can share the 
		same artefact (and no need to build it either)

	synchronizing jobs on different build nodes

	unique build ID
		generated on the machine where the build is started 

	setup CI job
		clone jobs
		different types of jobs
			support one type or let the jobs decide what they are
				what does Jenkins support and why?
					triggering by repo checkins

		scheduling
			scheduled at the trigger 
			rescheduled by maintainer
				pause, restart later (via cron)
				kill
			immediate if possible, after finding a free agent
 
	build under an alias and protect the build code 
		IE: user without rights can build projects that need special access to repo
			find a node in the cluster that can build as an alias
				parallel
					ssh to node
						include path to special jobs
						run can_build_XXX_protected
						
						test dependencies
							tools
							docker images
		
						cpu, disk, ...  availability
						

				decide where to be build
					or queue (can be handled by the trigger)

				build
					generate unique build ID

					ssh to node
						build self or ask a can_build?

						gather all necessary dependencies (tools, docker)

						wrap build system to catch output

						run build system
		
		
Workspaces per user Vs per cit
	all work is done as the user to be able to check credentials

	this means that the same build started by different users will have different Worspaces
		if a lot of users start the same job a lot of duplicates will exist

	Note that this is the right thing to do, there is no reason someone's build is share with someone else

	* the workdirs are called after the UUID
		but they can be under the build name

	if group builds, sharing worspaces, are needed, build as a group representative
		that needs to be a service, once that is accessible to people in the group only
		the workspaces are reachable as the users are part of the group



scheduling jobs on the nodes from the scheduler
	the job run as user but the user is not logged in and the scheduler should not
		run as root nor start the build as the user

	so the user, on the build node, has to wait till the scheduler allows it to run
		this means that the user could just run without scheduling
			* good or bad that the user has an account on the cluster?

	root could help control the users, not letting them run anything before the scheduler
	allows them to run a job (what's in the job can't be controlled, just how much it gets to run)


	job waits for a notification to start, a mail, a file in a directory accessible by cit scheduler?
		cit runs the build, in UUID_directory, based on the meta data	

		who starts the waiting job?
			cit when it queues the job to the scheduler
			jobs are serialized  so they can be put in the wait queue in case of errors

		* we can probe all the shared directory to check who has job queued

		
statistics about build node usage
	needed to plan future capacity

	some of the data we need:
		load per machine
		load per job
		resource needed per job

		disk usage

	* just data extraction, stats done by someone else


workspace retention:
	building for ourselves we can cleanup our own worskpaces

	what if there is not enough disk space and the other workspaces are not under our control?
		start a cleanup job per user from the scheduler


how do we handle "latest"?
	*** builds that use latest of some repo should be killed, optionally, if a new latest pops up

	so we need to keep a list of the running jobs or poll the build nodes




 


backup of scheduling data and build data
	
		



